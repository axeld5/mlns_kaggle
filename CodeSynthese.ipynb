{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook is made to synthetize the code of the kaggle competition hosted for the MLNS course. The competition was about performing link prediction on an actor network where nodes represent actors and edges between two nodes stand for co-occurrence on the same Wikipedia page."
      ],
      "metadata": {
        "id": "WX3NH7e3Zs-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilitary Functions\n",
        "\n",
        "Here are a list of functions that will remain relevant throughout all the notebook, that were used to help reading inputs and make submissions.\n",
        "\n",
        "Along those functions are given necessary installs so that there are no issues in running the notebook."
      ],
      "metadata": {
        "id": "6bjMp8R9bIYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4VUTOs5mcin",
        "outputId": "3e6f7a18-6199-490f-83ac-4854d7830808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.9/dist-packages (1.0.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (5.9.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.9/dist-packages (from dgl) (3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->dgl) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx \n",
        "import csv\n",
        "import torch\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "\n",
        "from typing import List, Dict\n",
        "\n",
        "def load_set(train=False) -> List[str]:\n",
        "    if train: \n",
        "        with open(\"train.txt\", \"r\") as f:\n",
        "            reader = csv.reader(f)\n",
        "            dataset = list(reader)\n",
        "        dataset = [element[0].split(\" \") for element in dataset]\n",
        "    else:\n",
        "        with open(\"test.txt\", \"r\") as f:\n",
        "            reader = csv.reader(f)\n",
        "            dataset = list(reader)\n",
        "        dataset = [element[0].split(\" \") for element in dataset]\n",
        "    return dataset\n",
        "\n",
        "def set_to_nx(dataset:List[str]) -> nx.Graph:\n",
        "    g = nx.Graph()\n",
        "    for elem in dataset:\n",
        "        g.add_node(elem[0])\n",
        "        g.add_node(elem[1])\n",
        "        if elem[2] == \"1\":\n",
        "            g.add_edge(elem[0], elem[1])\n",
        "    return g \n",
        "\n",
        "def set_to_directed_nx(dataset:List[str]) -> nx.Graph:\n",
        "    g = nx.DiGraph()\n",
        "    for elem in dataset:\n",
        "        g.add_node(elem[0])\n",
        "        g.add_node(elem[1])\n",
        "        if elem[2] == \"1\":\n",
        "            g.add_edge(elem[0], elem[1])\n",
        "    return g \n",
        "\n",
        "def get_non_edges(dataset:List[str]) -> List[str]:\n",
        "    non_edge_list = []\n",
        "    for elem in dataset:\n",
        "        if elem[2] == \"0\":\n",
        "            non_edge_list.append([elem[0], elem[1]])\n",
        "    return non_edge_list\n",
        "\n",
        "def info_to_dict() -> Dict[str, torch.Tensor]:\n",
        "    node_information = pd.read_csv(\"node_information.csv\", sep=\",\", header=None)\n",
        "    node_names = node_information[node_information.columns[0]]\n",
        "    node_information.drop(node_information.columns[0], axis=1, inplace=True)\n",
        "    node_data = torch.from_numpy(node_information.to_numpy()).to(torch.float32)\n",
        "    node_dict = {}\n",
        "    for i, idx in enumerate(node_names):\n",
        "        node_dict[str(idx)] = node_data[i].to_sparse()\n",
        "    return node_dict\n",
        "\n",
        "def generate_samples(graph:nx.Graph, non_edges:List[str], train_set_ratio:float):\n",
        "    #function taken from the MLNS practical sessions\n",
        "    if nx.is_connected(graph) is not True:\n",
        "        raise ValueError(\"The graph contains more than one connected component!\")\n",
        "    residual_g = graph.copy()\n",
        "    valid_pos_samples = []\n",
        "    edges = list(residual_g.edges())\n",
        "    np.random.shuffle(edges)\n",
        "    valid_set_size = int((1.0 - train_set_ratio) * graph.number_of_edges())\n",
        "    train_set_size = graph.number_of_edges() - valid_set_size\n",
        "    num_of_pos_valid_samples = 0\n",
        "    for edge in edges:\n",
        "        residual_g.remove_edge(edge[0], edge[1])\n",
        "        if nx.is_connected(residual_g):\n",
        "            num_of_pos_valid_samples += 1\n",
        "            valid_pos_samples.append(edge)\n",
        "        else: \n",
        "            residual_g.add_edge(edge[0], edge[1])\n",
        "        if num_of_pos_valid_samples == valid_set_size:\n",
        "            break\n",
        "    if num_of_pos_valid_samples != valid_set_size:\n",
        "        raise ValueError(\"Enough positive edge samples could not be found!\")\n",
        "    train_pos_samples = list(residual_g.edges())\n",
        "    np.random.shuffle(non_edges)\n",
        "    train_neg_samples = non_edges[:train_set_size] \n",
        "    valid_neg_samples = non_edges[train_set_size:train_set_size + valid_set_size]\n",
        "    train_samples = train_pos_samples + train_neg_samples\n",
        "    train_labels = [1 for _ in train_pos_samples] + [0 for _ in train_neg_samples]\n",
        "    # For valid set\n",
        "    valid_samples = valid_pos_samples + valid_neg_samples\n",
        "    valid_labels = [1 for _ in valid_pos_samples] + [0 for _ in valid_neg_samples]\n",
        "    return residual_g, train_samples, train_labels, valid_samples, valid_labels\n",
        "\n",
        "def create_submission(n_test:int, predictions:np.ndarray, pred_name:str):\n",
        "    submission = zip(np.arange(n_test), predictions)\n",
        "    # note: Kaggle requires that you add \"ID\" and \"category\" column headers\n",
        "    csv_name = pred_name+\".csv\"\n",
        "    with open(csv_name,\"w\") as pred:\n",
        "        csv_out = csv.writer(pred)\n",
        "        csv_out.writerow(i for i in [\"ID\", \"Predicted\"])\n",
        "        for row in submission:\n",
        "            csv_out.writerow(row)\n",
        "        pred.close()"
      ],
      "metadata": {
        "id": "K9x7zuWvbQBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use of Supervised Learning on Graph Features"
      ],
      "metadata": {
        "id": "ArNF460RZ83W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction\n",
        "\n",
        "The first approach tried was a traditional machine learning one, using global features of the graph extracted through networkx algorithms. Here are all the node features we use: \n",
        "- Degree centrality\n",
        "- Betweenness centrality\n",
        "- Pagerank\n",
        "- HITS\n",
        "- Katz Centrality\n",
        "\n",
        "As those features are node-focused and we want to keep on the fact that prediction should be done symetrically due to the graph being undirected (the classifier should predict (a, b) whether we feed it (b, a) or (a, b) as an edge), we take as features the average and quadratic distance between source and target node features.\n",
        "\n",
        "In addition, we add one global edge feature: Simrank.\n",
        "\n",
        "All those features are normalized using a MinMaxScaler. "
      ],
      "metadata": {
        "id": "l17shT9hdXfn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89OtYNrPZYDZ"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import networkx as nx \n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, scaler=MinMaxScaler()) -> None:\n",
        "        self.scaler = scaler\n",
        "\n",
        "    def feature_extract(self, graph, samples, train):\n",
        "        \"\"\"\n",
        "        Creates a feature vector for each edge of the graph contained in samples \n",
        "        \"\"\"\n",
        "        feature_vector = [] \n",
        "        \n",
        "        deg_centrality = nx.degree_centrality(graph)\n",
        "        betweeness_centrality = nx.betweenness_centrality(graph)\n",
        "        p_rank = nx.pagerank(graph)\n",
        "        hits = nx.hits(graph)\n",
        "        hubs = hits[0]\n",
        "        auth = hits[1]\n",
        "        simrank = nx.simrank_similarity(graph)\n",
        "        phi = np.abs(max(nx.adjacency_spectrum(graph)))\n",
        "        alpha = (1 / phi - 0.01)\n",
        "        katz = nx.katz_centrality(graph, alpha)\n",
        "\n",
        "        for edge in tqdm.tqdm(samples):\n",
        "            source_node, target_node = edge[0], edge[1]\n",
        "            \n",
        "            ## Global FEATURES\n",
        "            # Degree Centrality\n",
        "            src_dg_ctrl = deg_centrality[source_node]\n",
        "            tgt_dg_ctrl = deg_centrality[target_node]\n",
        "            dg_avg = (src_dg_ctrl+tgt_dg_ctrl)/2 \n",
        "            dg_std = (src_dg_ctrl-tgt_dg_ctrl)**2\n",
        "            \n",
        "            src_btw_ctrl = betweeness_centrality[source_node]\n",
        "            tgt_btw_ctrl = betweeness_centrality[target_node]\n",
        "            btw_avg = (src_btw_ctrl+tgt_btw_ctrl)/2 \n",
        "            btw_std = (src_btw_ctrl-tgt_btw_ctrl)**2\n",
        "\n",
        "            #Auth\n",
        "            src_hubs, tgt_hubs = hubs[source_node], hubs[target_node]\n",
        "            hubs_avg, hubs_std = (src_hubs+tgt_hubs)/2, (src_hubs - tgt_hubs)**2\n",
        "            src_auth, tgt_auth = auth[source_node], auth[target_node]\n",
        "            auth_avg, auth_std = (src_auth+tgt_auth)/2, (src_auth - tgt_auth)**2\n",
        "\n",
        "            #prank\n",
        "            src_p_rank = p_rank[source_node]\n",
        "            tgt_p_rank = p_rank[target_node]\n",
        "            p_rank_avg = (src_p_rank + tgt_p_rank)/2\n",
        "            p_rank_std = (src_p_rank - tgt_p_rank)**2\n",
        "\n",
        "            #katz \n",
        "            src_katz = katz[source_node]\n",
        "            tgt_katz = katz[target_node]\n",
        "            katz_avg = (src_katz + tgt_katz)/2 \n",
        "            katz_std = (src_katz - tgt_katz)**2\n",
        "\n",
        "            #simrank \n",
        "            edge_simrank = simrank[source_node][target_node]\n",
        "\n",
        "            #features = np.concatenate((vect_features, emb_features))\n",
        "            features = np.array([dg_avg, dg_std, btw_avg, btw_std, hubs_avg, hubs_std, auth_avg, auth_std, p_rank_avg, p_rank_std, katz_avg, katz_std, edge_simrank])\n",
        "            feature_vector.append(features) \n",
        "        feature_vector = np.array(feature_vector)\n",
        "        if train:\n",
        "            feature_vector = self.scaler.fit_transform(feature_vector)\n",
        "        else:\n",
        "            feature_vector = self.scaler.transform(feature_vector)\n",
        "        return feature_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifier\n",
        "\n",
        "Once those features are extracted, a Logistic Regression is performed to realize the classification tasks. Parameters (C, L1_ratio, penalty, solver) were found using Grid Search Cross Validation.\n",
        "\n",
        "Other algorithms that were tried were RandomForest and XGBoost, but in practice, they performed less better than LogisticRegression in our use case."
      ],
      "metadata": {
        "id": "RKSowGgbcG5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "train_set = load_set(train=True)\n",
        "g = set_to_nx(train_set)\n",
        "non_edges = get_non_edges(train_set)\n",
        "extractor = FeatureExtractor()\n",
        "clf = LogisticRegression(C=100, l1_ratio=0.1, penalty=\"elasticnet\", solver=\"saga\", max_iter=10000)\n",
        "evaluate_method = False\n",
        "if evaluate_method:\n",
        "        residual_g, train_samples, train_labels, valid_samples, valid_labels = generate_samples(g, non_edges, 0.8)\n",
        "        train_features = extractor.feature_extract(residual_g, train_samples, train=True)\n",
        "        valid_features = extractor.feature_extract(residual_g, valid_samples, train=False)\n",
        "        param_dict = {}\n",
        "        param_dict[\"penalty\"] = [\"elasticnet\"]\n",
        "        param_dict[\"solver\"] = [\"saga\"]\n",
        "        param_dict[\"max_iter\"] = [10000]\n",
        "        param_dict[\"C\"] = [0.1, 1, 10, 100, 1000]\n",
        "        param_dict[\"l1_ratio\"] = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "        clf = GridSearchCV(clf, param_dict, verbose=True)\n",
        "        clf.fit(train_features, train_labels)\n",
        "        print(clf.score(valid_features, valid_labels))\n",
        "        print(clf.best_params_)   \n",
        "else:\n",
        "        total_samples = list(g.edges) + non_edges \n",
        "        total_labels = [1 for _ in range(len(g.edges))] + [0 for _ in range(len(non_edges))]\n",
        "        total_features = extractor.feature_extract(g, total_samples, train=True)\n",
        "        clf.fit(total_features, total_labels)\n",
        "        test_set = load_set(train=False)\n",
        "        n_test = len(test_set)\n",
        "        test_features = extractor.feature_extract(g, test_set, train=False) \n",
        "        pred = clf.predict(test_features)\n",
        "        create_submission(n_test, pred, pred_name=\"submissions/supervised_pred\")"
      ],
      "metadata": {
        "id": "2p41sRlha6ah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5802d075-fc95-4028-d480-9d01f6c1b197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10496/10496 [00:00<00:00, 130313.87it/s]\n",
            "100%|██████████| 3498/3498 [00:00<00:00, 79707.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction using Edge Features Ranking"
      ],
      "metadata": {
        "id": "4bHpdk-CdOa3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction\n",
        "\n",
        "Another approach that was tried was an unsupervised one, using Edge Features that are commonly used for Link Prediction as a way to rank the more likely edges in the test set to be connected.\n",
        "\n",
        "The features that were used are the following:\n",
        "- Common Neighbor Centrality\n",
        "- Preferential Attachment\n",
        "- Jaccard Coefficient\n",
        "- Ressource Allocation Index\n",
        "- Adamic Adar Index\n",
        "- Shortest Path Length between target node and source node\n",
        "\n",
        "At first, those features were normalized, but we noticed that it was not necessary to do so, as we only cared about how each edge was ranked relatively to each feature."
      ],
      "metadata": {
        "id": "GVuVC0KTdhkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import networkx as nx \n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, scaler=MinMaxScaler()) -> None:\n",
        "        self.scaler = scaler\n",
        "\n",
        "    def feature_extract(self, graph, samples, train):\n",
        "        \"\"\"\n",
        "        Creates a feature vector for each edge of the graph contained in samples \n",
        "        \"\"\"\n",
        "        feature_vector = [] \n",
        "        epured_samples = []\n",
        "        for edge in samples:\n",
        "            if edge[0] != edge[1]:\n",
        "                epured_samples.append(edge)\n",
        "        cnc = nx.common_neighbor_centrality(graph, epured_samples) \n",
        "        cnc_list = list(cnc)   \n",
        "        for edge in tqdm.tqdm(samples):\n",
        "            source_node, target_node = edge[0], edge[1]\n",
        "\n",
        "            ## EDGE FEATURES\n",
        "            # Preferential Attachement \n",
        "            pref_attach = list(nx.preferential_attachment(graph, [(source_node, target_node)]))[0][2]\n",
        "            # Jaccard\n",
        "            jaccard_coeff = list(nx.jaccard_coefficient(graph, [(source_node, target_node)]))[0][2]\n",
        "            # Ressource Allocation Index\n",
        "            rai = list(nx.resource_allocation_index(graph, [(source_node, target_node)]))[0][2]\n",
        "            # Adamic Adar Index \n",
        "            aai = 0 \n",
        "            if target_node != source_node: \n",
        "                aai = list(nx.adamic_adar_index(graph, [(source_node, target_node)]))[0][2]\n",
        "            # Common Neighbor Centrality\n",
        "            cnc = 0\n",
        "            if target_node != source_node:\n",
        "                for elem in cnc_list: \n",
        "                    if elem[0] == source_node and elem[1] == target_node:\n",
        "                        cnc = elem[2] \n",
        "                        continue\n",
        "            # Shortest Path\n",
        "            sp = nx.shortest_path_length(graph, source_node, target_node)\n",
        "\n",
        "            features = np.array([jaccard_coeff, rai, pref_attach, aai, cnc, sp])\n",
        "            feature_vector.append(features) \n",
        "        feature_vector = np.array(feature_vector)\n",
        "        return feature_vector\n",
        "        "
      ],
      "metadata": {
        "id": "er4jyojhdg35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction\n",
        "\n",
        "Under the assumption that there are as many edges as non-edges in the test set (which is usually a strong assumption, but seems to be the case for this competition !), we take the half of the edges with the better average rank.\n",
        "\n",
        "As actually a lot of edges are between nodes that do not have common neighbors, Adamic Adar, RAI and Jaccard are often equal to 0. Thus, in our ranking, the final submission used the \"min\" method of pandas in order to bring out less those nodes compared to those with an AA, RAI and Jaccard that are strictly positive."
      ],
      "metadata": {
        "id": "6uT-0Sndegvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "\n",
        "train_set = load_set(train=True)\n",
        "g = set_to_nx(train_set)\n",
        "extractor = FeatureExtractor()\n",
        "test_set = load_set(train=False)\n",
        "n_test = len(test_set)\n",
        "test_features = extractor.feature_extract(g, test_set, True)\n",
        "test_features = pd.DataFrame(test_features)\n",
        "test_ranks = test_features.rank(axis=0, method=\"min\")\n",
        "test_features = np.mean(test_ranks.to_numpy(), axis=1)\n",
        "sorted_features = np.argsort(test_features)[::-1]\n",
        "pred = np.zeros(n_test)\n",
        "for i in range(int(len(sorted_features)*0.5)):\n",
        "    idx = sorted_features[i]\n",
        "    pred[idx] = 1\n",
        "create_submission(n_test, pred, pred_name=\"unsupervised_pred\")"
      ],
      "metadata": {
        "id": "tIAwNbznee0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a581e38a-2f29-4fcc-9886-58a91c657abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3498/3498 [00:01<00:00, 2547.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph Neural Network to make prediction based on Node Information"
      ],
      "metadata": {
        "id": "P6idccVbfQs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Idea\n",
        "\n",
        "Node Information was a tough feature to exploit. It was unusable for both previous types of predictors, as it often made predictions worse if it was put alongside the other features.\n",
        "\n",
        "The idea we had in the end was using a Graph Neural Network to extract embeddings from the Node features using the Graph structure that was provided, and then make prediction using an MLP based on those features.\n",
        "\n",
        "Two things have to be noted here:\n",
        "- As DGL turns any undirected graph into a directed graph, we had to remove edges that were in double in order to not bias the training process.\n",
        "- As we want to keep symmetry between predictions (results should not change for (a, b) and (b, a)) we feed to the MLP predictor the average and quadratic distance between source and target embeddings output by the Graph Neural Network.\n",
        "\n",
        "The Neural Network that was used here was GraphSage."
      ],
      "metadata": {
        "id": "ZSCyBlO_fZlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Networks"
      ],
      "metadata": {
        "id": "lXADWZnHfb2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from dgl.nn import SAGEConv\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, output_feats, n_layers, dropout, skip=False):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        first_conv = SAGEConv(in_feats, h_feats, 'mean')\n",
        "        self.convs.append(first_conv)\n",
        "        self.n_layers = n_layers\n",
        "        for i in range(n_layers - 2):\n",
        "            add_conv = SAGEConv(h_feats, h_feats, 'mean')\n",
        "            self.convs.append(add_conv)\n",
        "        last_conv = SAGEConv(h_feats, output_feats, 'mean')\n",
        "        self.convs.append(last_conv)\n",
        "        self.dropout = dropout\n",
        "        self.skip = skip\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        prev_h = None\n",
        "        h = self.convs[0](g, in_feat)\n",
        "        h = F.relu(h) \n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        for conv in self.convs[1:self.n_layers-1]:\n",
        "            prev_h = h\n",
        "            h = conv(g, h)            \n",
        "            if self.skip:\n",
        "                h = h + prev_h\n",
        "            h = F.relu(h)\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = self.convs[-1](g, h)\n",
        "        return h"
      ],
      "metadata": {
        "id": "DA9rm4HMmFZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import dgl.function as fn\n",
        "\n",
        "class MLPUpgradedPredictor(nn.Module):\n",
        "    def __init__(self, h_feats):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
        "        self.W2 = nn.Linear(h_feats, 1)\n",
        "\n",
        "    def apply_edges(self, edges):\n",
        "        \"\"\"\n",
        "        Computes a scalar score for each edge of the given graph.\n",
        "        Parameters\n",
        "        ----------\n",
        "        edges :\n",
        "            Has three members ``src``, ``dst`` and ``data``, each of\n",
        "            which is a dictionary representing the features of the\n",
        "            source nodes, the destination nodes, and the edges\n",
        "            themselves.\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            A dictionary of new edge features.\n",
        "        \"\"\"\n",
        "        avg = (edges.src['h']+edges.dst['h'])/2\n",
        "        var = (edges.src['h']-edges.dst['h'])**2\n",
        "        h = torch.cat([avg, var], 1)\n",
        "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            g.apply_edges(self.apply_edges)\n",
        "            return g.edata['score']"
      ],
      "metadata": {
        "id": "4WULSgacmI5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn.functional as F \n",
        "\n",
        "def compute_loss(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "def train(model, pred, train_g, train_pos_g, train_neg_g, optimizer, num_epochs=100):\n",
        "    model.train()\n",
        "    pred.train()\n",
        "    for e in range(num_epochs):\n",
        "        # forward\n",
        "        h = model(train_g, train_g.ndata['feat'])\n",
        "        pos_score = pred(train_pos_g, h)\n",
        "        neg_score = pred(train_neg_g, h)\n",
        "        loss = compute_loss(pos_score, neg_score)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if e % 5 == 0:\n",
        "            print('In epoch {}, loss: {}'.format(e, loss))"
      ],
      "metadata": {
        "id": "buaB7-y6muj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction"
      ],
      "metadata": {
        "id": "uioK_db8fexx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import roc_auc_score \n",
        "\n",
        "def evaluate_torch(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
        "    labels = torch.cat(\n",
        "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
        "    return roc_auc_score(labels, scores)"
      ],
      "metadata": {
        "id": "O3kIS84EnCVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import torch\n",
        "import itertools\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import dgl.data\n",
        "import networkx as nx\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #create dgl graph\n",
        "    train_set = load_set(train=True)\n",
        "    nx_g = set_to_directed_nx(train_set)\n",
        "    node_dict = info_to_dict()\n",
        "    nx.set_node_attributes(nx_g, values=node_dict, name=\"feat\")\n",
        "    sorted_nodes = sorted(nx_g.nodes())\n",
        "    #creating convert dict to keep node embeddings\n",
        "    convert_dict = {}\n",
        "    for i, node in enumerate(sorted_nodes):\n",
        "        convert_dict[node] = i\n",
        "    g = dgl.from_networkx(nx_g, node_attrs=[\"feat\"], idtype=torch.int32)\n",
        "\n",
        "    #get positive attributes\n",
        "    u, v = g.edges()\n",
        "    eval_model = False\n",
        "    if eval_model:\n",
        "        eids = np.arange(g.number_of_edges())\n",
        "        eids = np.random.permutation(eids)\n",
        "        test_size = int(len(eids) * 0.1)\n",
        "        train_size = g.number_of_edges() - test_size\n",
        "        test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
        "        train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
        "\n",
        "        # Find all known negative edges and split them for training and testing\n",
        "        non_edges = get_non_edges(train_set)\n",
        "        non_edges_indic = np.ones((g.number_of_nodes(), g.number_of_nodes()))\n",
        "        for elem in non_edges: \n",
        "            idx_0 = convert_dict[elem[0]] \n",
        "            idx_1 = convert_dict[elem[1]]\n",
        "            non_edges_indic[idx_0][idx_1] = 0\n",
        "            non_edges_indic[idx_1][idx_0] = 0\n",
        "        neg_u, neg_v = np.where(non_edges_indic == 0)\n",
        "        neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n",
        "        test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
        "        train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n",
        "        train_g = dgl.remove_edges(g, eids[:test_size])\n",
        "            \n",
        "        #create train and test graphs\n",
        "        train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
        "        train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
        "        test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
        "        test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n",
        "\n",
        "        #Define model, optimizer, and training step\n",
        "        model = GraphSAGE(train_g.ndata['feat'].shape[1], 64, 32, n_layers=3, dropout=0.2, skip=True)\n",
        "        pred = MLPUpgradedPredictor(32)\n",
        "        optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.005, weight_decay=5*10**-4)\n",
        "        train(model, pred, train_g, train_pos_g, train_neg_g, optimizer, num_epochs=250)\n",
        "\n",
        "        #make a test\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            pred.eval()\n",
        "            h = model(train_g, train_g.ndata['feat'])\n",
        "            pos_score = pred(test_pos_g, h)\n",
        "            neg_score = pred(test_neg_g, h)\n",
        "            print('AUC', evaluate_torch(pos_score, neg_score))\n",
        "\n",
        "    else: \n",
        "        train_pos_g = dgl.graph((u, v), num_nodes=g.number_of_nodes())\n",
        "        non_edges = get_non_edges(train_set)\n",
        "        non_edges_indic = np.ones((g.number_of_nodes(), g.number_of_nodes()))\n",
        "        for elem in non_edges: \n",
        "            idx_0 = convert_dict[elem[0]] \n",
        "            idx_1 = convert_dict[elem[1]]\n",
        "            non_edges_indic[idx_0][idx_1] = 0\n",
        "            non_edges_indic[idx_1][idx_0] = 0\n",
        "        neg_u, neg_v = np.where(non_edges_indic == 0)\n",
        "        train_neg_g = dgl.graph((neg_u, neg_v), num_nodes=g.number_of_nodes())\n",
        "        model = GraphSAGE(g.ndata['feat'].shape[1], 32, 16, n_layers=3, dropout=0.2, skip=True)\n",
        "        pred = MLPUpgradedPredictor(16)\n",
        "        optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.001)\n",
        "        train(model, pred, g, train_pos_g, train_neg_g, optimizer, num_epochs=250)\n",
        "\n",
        "        #make a test\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            pred.eval()\n",
        "            h = model(g, g.ndata['feat'])\n",
        "\n",
        "        #get submission\n",
        "        test_set = load_set(train=False)\n",
        "        test_u = np.zeros(len(test_set))\n",
        "        test_v = np.zeros(len(test_set))\n",
        "        for i in range(len(test_set)):\n",
        "            test_u[i] = convert_dict[test_set[i][0]]\n",
        "            test_v[i] = convert_dict[test_set[i][1]]\n",
        "        test_g = dgl.graph((test_u, test_v), num_nodes=g.number_of_nodes())\n",
        "        with torch.no_grad():\n",
        "            test_score = pred(test_g, h)\n",
        "        test_pred = np.zeros(test_score.shape[0])\n",
        "        for i, elem in enumerate(test_score):\n",
        "            if elem < 0:\n",
        "                test_pred[i] = 0\n",
        "            else:\n",
        "                test_pred[i] = 1\n",
        "        n_test = len(test_set)\n",
        "        create_submission(n_test, test_pred, pred_name=\"submissions/gnn\")"
      ],
      "metadata": {
        "id": "AOkINYTPfYsf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03474425-a876-4427-da74-6fff198bd5ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0, loss: 0.6832396984100342\n",
            "In epoch 5, loss: 0.666217029094696\n",
            "In epoch 10, loss: 0.6527720093727112\n",
            "In epoch 15, loss: 0.6400973796844482\n",
            "In epoch 20, loss: 0.6326701641082764\n",
            "In epoch 25, loss: 0.6260826587677002\n",
            "In epoch 30, loss: 0.614692211151123\n",
            "In epoch 35, loss: 0.6078561544418335\n",
            "In epoch 40, loss: 0.5978363156318665\n",
            "In epoch 45, loss: 0.585607647895813\n",
            "In epoch 50, loss: 0.5701477527618408\n",
            "In epoch 55, loss: 0.5505990386009216\n",
            "In epoch 60, loss: 0.530487596988678\n",
            "In epoch 65, loss: 0.503662109375\n",
            "In epoch 70, loss: 0.4856695234775543\n",
            "In epoch 75, loss: 0.4582064747810364\n",
            "In epoch 80, loss: 0.437043696641922\n",
            "In epoch 85, loss: 0.4263246953487396\n",
            "In epoch 90, loss: 0.405521422624588\n",
            "In epoch 95, loss: 0.401401162147522\n",
            "In epoch 100, loss: 0.38530537486076355\n",
            "In epoch 105, loss: 0.36958053708076477\n",
            "In epoch 110, loss: 0.3479953110218048\n",
            "In epoch 115, loss: 0.3397076427936554\n",
            "In epoch 120, loss: 0.3389747142791748\n",
            "In epoch 125, loss: 0.3325619101524353\n",
            "In epoch 130, loss: 0.3161757290363312\n",
            "In epoch 135, loss: 0.3049547076225281\n",
            "In epoch 140, loss: 0.2929455041885376\n",
            "In epoch 145, loss: 0.2953711152076721\n",
            "In epoch 150, loss: 0.283719539642334\n",
            "In epoch 155, loss: 0.2835138142108917\n",
            "In epoch 160, loss: 0.2683221399784088\n",
            "In epoch 165, loss: 0.2619353234767914\n",
            "In epoch 170, loss: 0.2548444867134094\n",
            "In epoch 175, loss: 0.24570587277412415\n",
            "In epoch 180, loss: 0.24253183603286743\n",
            "In epoch 185, loss: 0.2427481859922409\n",
            "In epoch 190, loss: 0.23221413791179657\n",
            "In epoch 195, loss: 0.22646555304527283\n",
            "In epoch 200, loss: 0.2186916321516037\n",
            "In epoch 205, loss: 0.21650265157222748\n",
            "In epoch 210, loss: 0.20600584149360657\n",
            "In epoch 215, loss: 0.208602637052536\n",
            "In epoch 220, loss: 0.19779671728610992\n",
            "In epoch 225, loss: 0.19813843071460724\n",
            "In epoch 230, loss: 0.2022675722837448\n",
            "In epoch 235, loss: 0.189591184258461\n",
            "In epoch 240, loss: 0.1931685507297516\n",
            "In epoch 245, loss: 0.18820662796497345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use of Graph Clustering to add information"
      ],
      "metadata": {
        "id": "bHG6O1tbfiFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Idea\n",
        "\n",
        "A last predictor that was explored is a simple one, that still gives information: use of graph clustering methods in order to identify communities.\n",
        "\n",
        "The link prediction happens the following way: if the two nodes involved are part of the same community, say there will be an edge. Otherwise, say there won't be an edge.\n",
        "\n",
        "While this method is naïve, it increases the final score when paired with other methods. What happens is that it does not predict many edges in the end, but it can be expected that the ones that are predicted make sense.\n",
        "\n",
        "The clustering method that was used was the Clauset-Newman-Moore modularity maximization to find the community partition with the largest modularity, that can be found implemented in networkx."
      ],
      "metadata": {
        "id": "6iDHIbKPfooV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx \n",
        "\n",
        "from networkx.algorithms.community import greedy_modularity_communities\n",
        "\n",
        "def get_community_based_pred(g, test_set):\n",
        "    c = greedy_modularity_communities(g)\n",
        "    comm_dict = get_community_dict(g, c)\n",
        "    comm_pred = get_pred(comm_dict, test_set)\n",
        "    return comm_pred\n",
        "\n",
        "def get_community_dict(g, c):\n",
        "    comm_dict = {}\n",
        "    for i, community in enumerate(c):  \n",
        "        for node in g.nodes: \n",
        "            if node in community:\n",
        "                comm_dict[node] = i\n",
        "    return comm_dict\n",
        "\n",
        "def get_pred(comm_dict, test_set):\n",
        "    comm_pred = np.zeros(len(test_set))\n",
        "    for i, elem in enumerate(test_set):\n",
        "        src, tgt = elem[0], elem[1]\n",
        "        comm_src = comm_dict[src]\n",
        "        comm_tgt = comm_dict[tgt]\n",
        "        if comm_src == comm_tgt:\n",
        "            comm_pred[i] = 1\n",
        "    return comm_pred"
      ],
      "metadata": {
        "id": "Azj3_H4toIqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction"
      ],
      "metadata": {
        "id": "r8nKh-blfrH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = load_set(train=True)\n",
        "g = set_to_nx(train_set)\n",
        "test_set = load_set(train=False)\n",
        "n_test = len(test_set)\n",
        "community_pred = get_community_based_pred(g, test_set)\n",
        "create_submission(n_test, community_pred, pred_name=\"submissions/community_pred\")"
      ],
      "metadata": {
        "id": "i_LldUmqfk9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Prediction : Majority Vote"
      ],
      "metadata": {
        "id": "gKFX76MEfuWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Idea\n",
        "\n",
        "The idea to make the final submissions was to exploit all information provided by the different methods that were tested:\n",
        "- Supervised learning prediction performs link prediction using several global features.\n",
        "- Unsupervised prediction performs link prediction using several local features.\n",
        "- Deep learning prediction performs link prediction using node information.\n",
        "- Graph clustering performs some more advanced but less complete prediction using the graph structure.\n",
        "\n",
        "Therefore, in order to predict whether or not a link between two nodes will happen, the four models make a prediction. If two models happen to tell that this edge will happen, then it is marked as true. This method is the one that scored best on the Kaggle competition."
      ],
      "metadata": {
        "id": "RXFtUV2UoqE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction"
      ],
      "metadata": {
        "id": "7ds-DODLoq-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "\n",
        "unsup_pred = pd.read_csv(\"submissions/unsupervised_pred.csv\").to_numpy()\n",
        "gnn_pred = pd.read_csv(\"submissions/gnn.csv\").to_numpy()\n",
        "sup_pred = pd.read_csv(\"submissions/supervised_pred.csv\").to_numpy() \n",
        "comm_pred = pd.read_csv(\"submissions/community_pred.csv\").to_numpy()\n",
        "final_pred = (0.25*sup_pred+0.25*unsup_pred+0.25*gnn_pred+0.25*comm_pred)[:, 1]\n",
        "for i in range(len(final_pred)):\n",
        "    if final_pred[i] >= 0.5:\n",
        "        final_pred[i] = 1\n",
        "    else:\n",
        "        final_pred[i] = 0\n",
        "n_test = len(final_pred)\n",
        "create_submission(n_test, final_pred, \"submissions/avgd\")"
      ],
      "metadata": {
        "id": "d0fOOnL-fwo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Methods that were tried but failed\n",
        "\n",
        "In this section, we report several methods that were tried but did not yield in an increase of results. All those methods can still be found coded within the zip folder sent with the report."
      ],
      "metadata": {
        "id": "54YmmjMOf2j4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Link Prediction\n",
        "\n",
        "Combining local features and global features in the supervised predictor did not yield better performances.\n",
        "\n",
        "Node2vec and Word2vec node embeddings were not good predictors at all in this use case, no matter the tuning of hyperparameters that was done.\n",
        "\n",
        "Putting the node information into the predictors was not effective and reduced performance, no matter how it was put. We have tried:\n",
        "- putting the full information vector into the classifier\n",
        "-  putting a reduced information vector, whether it was through PCA or Embedded through a Graph Neural Network trained for Link Prediction like in the \"Graph Neural Network\" section\n",
        "- using cosine similary on the information vectors in order to have symmetrical information\n",
        "\n",
        "None of those methods resulted in an increase in performance."
      ],
      "metadata": {
        "id": "thVUfKV4qZFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph Neural Networks based-approach\n",
        "\n",
        "Using Simply a DotProduct predictor instead of a Neural Network resulted in far worse predictions.\n",
        "\n",
        "VGNAE was tried as well: the idea was to mimic autoencoders for Graphs and recreate the adjacency matrix through latent space embedding, and use that to make predictions. While the method is extremely performant on datasets like Cora or Pubmed, it seems to extremely underperform in our use case. This may have been due to difficulties in hyperparameter tuning, or due to the fact the graph we have through the train.txt is not fit for this type of network.\n",
        "\n"
      ],
      "metadata": {
        "id": "xYQ05B5Drsrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering-based approach\n",
        "\n",
        "Spectral Clustering was tried to perform link prediction like we've done with modularity maximization, but it required a too high k value to create significative communities. Even then, the communities it created were less significant than the communities created through modularity maximization. Therefore, we have dropped this approach."
      ],
      "metadata": {
        "id": "pYWnSaVQsWAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "Overall, this competition was a great opportunity to experiment on graph-based link prediction."
      ],
      "metadata": {
        "id": "FqbMlSGgfxPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "Graph features taxonomy: https://mdpi-res.com/d_attachment/make/make-02-00036/article_deploy/make-02-00036-v2.pdf?version=1608552901\n",
        "\n",
        "Idea on how to use GraphSage for feature extraction was taken from: https://medium.com/stanford-cs224w/gnn-based-link-prediction-in-drug-drug-interaction-networks-c0e2136e4a72\n",
        "\n",
        "Idea to use VGNAE: https://arxiv.org/pdf/2108.08046.pdf"
      ],
      "metadata": {
        "id": "UPHrS998s9Qx"
      }
    }
  ]
}